<div data-citation-items="%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F9025336%2Fitems%2FITY7GKUZ%22%5D%2C%22itemData%22%3A%7B%22id%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F9025336%2Fitems%2FITY7GKUZ%22%2C%22type%22%3A%22report%22%2C%22abstract%22%3A%22We%20introduce%20the%20variational%20graph%20auto-encoder%20(VGAE)%2C%20a%20framework%20for%20unsupervised%20learning%20on%20graph-structured%20data%20based%20on%20the%20variational%20auto-encoder%20(VAE).%20This%20model%20makes%20use%20of%20latent%20variables%20and%20is%20capable%20of%20learning%20interpretable%20latent%20representations%20for%20undirected%20graphs.%20We%20demonstrate%20this%20model%20using%20a%20graph%20convolutional%20network%20(GCN)%20encoder%20and%20a%20simple%20inner%20product%20decoder.%20Our%20model%20achieves%20competitive%20results%20on%20a%20link%20prediction%20task%20in%20citation%20networks.%20In%20contrast%20to%20most%20existing%20models%20for%20unsupervised%20learning%20on%20graph-structured%20data%20and%20link%20prediction%2C%20our%20model%20can%20naturally%20incorporate%20node%20features%2C%20which%20significantly%20improves%20predictive%20performance%20on%20a%20number%20of%20benchmark%20datasets.%22%2C%22note%22%3A%221751%20citations%20(Semantic%20Scholar%2FarXiv)%20%5B2023-01-24%5D%5CnDOI%3A%2010.48550%2FarXiv.1611.07308%5CnarXiv%3A1611.07308%20%5Bcs%2C%20stat%5D%5Cntype%3A%20article%5CnCitation%20Key%3A%20kipfVariationalGraphAutoEncoders2016%22%2C%22number%22%3A%22arXiv%3A1611.07308%22%2C%22publisher%22%3A%22arXiv%22%2C%22source%22%3A%22arXiv.org%22%2C%22title%22%3A%22Variational%20Graph%20Auto-Encoders%22%2C%22URL%22%3A%22http%3A%2F%2Farxiv.org%2Fabs%2F1611.07308%22%2C%22author%22%3A%5B%7B%22family%22%3A%22Kipf%22%2C%22given%22%3A%22Thomas%20N.%22%7D%2C%7B%22family%22%3A%22Welling%22%2C%22given%22%3A%22Max%22%7D%5D%2C%22accessed%22%3A%7B%22date-parts%22%3A%5B%5B%222022%22%2C6%2C15%5D%5D%7D%2C%22issued%22%3A%7B%22date-parts%22%3A%5B%5B%222016%22%2C11%2C21%5D%5D%7D%2C%22citation-key%22%3A%22kipfVariationalGraphAutoEncoders2016%22%7D%7D%5D" data-schema-version="8"><h1>Zotero Markdown Notes<br/>(1/26/2023, 10:04:09 AM)</h1>\n<p><img data-attachment-key="BZJA8E94" width="342" height="343" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F9025336%2Fitems%2FUIVEMQEC%22%2C%22annotationKey%22%3A%22W9T85QP4%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%221%22%2C%22position%22%3A%7B%22pageIndex%22%3A0%2C%22rects%22%3A%5B%5B303.4820000000002%2C326.935%2C508.55048616305163%2C532.461%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F9025336%2Fitems%2FITY7GKUZ%22%5D%2C%22locator%22%3A%221%22%7D%7D"/><br/><span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F9025336%2Fitems%2FITY7GKUZ%22%5D%2C%22locator%22%3A%221%22%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Kipf and Welling, 2016, p. 1</span>)</span> This a great image</p><blockquote><span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F9025336%2Fitems%2FUIVEMQEC%22%2C%22annotationKey%22%3A%222R7IFRTY%22%2C%22color%22%3A%22%232ea8e5%22%2C%22pageLabel%22%3A%221%22%2C%22position%22%3A%7B%22pageIndex%22%3A0%2C%22rects%22%3A%5B%5B108%2C521.83%2C166.654%2C530.737%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F9025336%2Fitems%2FITY7GKUZ%22%5D%2C%22locator%22%3A%221%22%7D%7D">We introduce</span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F9025336%2Fitems%2FITY7GKUZ%22%5D%2C%22locator%22%3A%221%22%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Kipf and Welling, 2016, p. 1</span>)</span></blockquote><ul>A comment on the intro</ul><p>#regular</p>\n</div>